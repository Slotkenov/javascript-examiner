% (product 2) Checks #59
% 
% Here the four checks should be discussed
% - functionality
% - format
% - maintainability
% - syntax

% Algemeen
As it stands, four checks are part of the \gls{examiner}.
The number of checks can easily be extended due to the loose coupling as already
in the Architecture section.

\section{Format}

The format of the JavaScript within the solution is checked with the standard
% Voetnoot link naar JSCS website
JavaScript Code Style (JSCS) code style checker\footnote{\url{http://jscs.info/}}.
It's configuration can be set through a configuration file, jscs-config.json.
% Linken naar de google preset config file
Currently the content of this file is {"preset": "google"} which sets the checker
to the Google preset.
This can be configured at will, depending on the wishes of the tutor.
The Google preset is a reflection of the style guide and is a list of restrictions
for \gls{js-code}\footnote{\url{https://google-styleguide.googlecode.com/svn/trunk/javascriptguide.xml}}.
JavaScript is the main client-side scripting language used by many of Google's
open-source projects.


\section{Functionality}
% Onveiligheid beschrijven van executen (ongeveriefierde code)

This \gls{check} uses the Mocha framework \footnote{\url{http://mochajs.org/}} 
to run tests on the submitted code. The \gls{tutor} adds a testsuite to the
exercise. When the \gls{student} submits a \gls{solution}, the \gls{examiner}
checks whether there is a testsuite available for the relating \gls{exercise}.
If this is the case, the \gls{solution} will be transformed to a module, and
included in the ``in place'' generated testsuite module. This way, we can test
the functionality of the \gls{solution} server side. The \gls{solution} is
actually being executed. As this code is in no way controllable, it might be
possible to crash the tool by submitting a \gls{solution} consisting of 
malicious \gls{js-code}. This has been recognized as a serious threat early on,
and thoroughly discussed with the product owner. After addressing this threat 
explicitly, it was agreed to implement it this way.

While designing this \gls{check}, other and saver approaches passed. They where
to complicated to implement during our project, but may be useful when improving
the safety of this check, or extending it's features. Moving the executing of the
testsuite to the client side, resolves the risk of application failure due to
malicious code submitted by \glspl{student}. On the downside of this solution, it would
no longer be possible to guarantee the authenticity of the results, when
transmitted back to the server for the purpose of student progress logging. 
A more robust implementation can be achieved alike the approach described in a
blog of remoteinterview.io.\footnote{\url{http://blog.remoteinterview.io/post/89639823776/how-we-used-docker-to-compile-and-run-untrusted/}}
Their approach is using docker\footnote{\url{http://www.docker.com/}}, a tool to
run virtual server sandboxes, to create a failsafe environment wherein the 
potentially malicious can be run without the risk of harming the main server or
even the executing application. This approach seems to overcome the safety risk,
without losing functionality. 

\section{Maintainability}

The maintainability \gls{check} uses esprima\footnote{\url{http://esprima.org/}},
escomplex\footnote{\url{https://github.com/philbooth/escomplex}} and
escomplex-ast-moz\footnote{\url{https://github.com/philbooth/escomplex-ast-moz}} module.
The first module generates output that is used as input for the next module.
Its output is send to the third and the total output is the software
complexity analysis of the submitted code described in a number of metrics.

The maintainability check is a check to generate metrics about the submitted code.
Currently the metrics the library reports on are :
\begin{enumerate}
 \item Lines of code: Both physical (the number of lines in a module or function) and logical (a count of the imperative statements). A crude measure.
 \item Number of parameters: Analysed statically from the function signature, so no accounting is made for functions that rely on the arguments object. Lower is better.
 \item Cyclomatic complexity: Defined by Thomas J. McCabe in 1976, this is a count of the number of cycles in the program flow control graph. Effectively the number of distinct paths through a block of code. Lower is better.
 \item Cyclomatic complexity density: Proposed as a modification to cyclomatic complexity by Geoffrey K. Gill and Chris F. Kemerer in 1991, this metric simply re-expresses it as a percentage of the logical lines of code. Lower is better.
 \item Halstead metrics: Defined by Maurice Halstead in 1977, these metrics are calculated from the numbers of operators and operands in each function. Lower is better.
 \item Maintainability index: Defined by Paul Oman \& Jack Hagemeister in 1991, this is a logarithmic scale from negative infinity to 171, calculated from the logical lines of code, the cyclomatix complexity and the Halstead effort. Higher is better.
 \item Dependencies: A count of the calls to CommonJS and AMD require. Analysed statically from the function signature, so no accounting is made for dynamic calls where a variable or function is obscuring the nature of the dependency. Lower is better.
 \item First-order density: The percentage of all possible internal dependencies that are actually realised in the project. Lower is better.
 \item Change cost: The percentage of modules affected, on average, when one module in the project is changed. Lower is better.
 \item Core size: The percentage of modules that are both widely depended on and themselves depend on other modules. Lower is better.
\end{enumerate}

As stated by the author : ``It is important to note that none of these metrics can compete with the insight of a competent developer. At best, they are an automatable warning system, which can help to identify areas of code that warrant closer inspection by a human being.''

If these metrics are persisted along with the submitted code, it can be used for
analyzation purposes.

The module can be called with a number of options set.
Default these options are set to false.
In the \gls{check} module, the options are manually set.
The tolerant option is set to false. This means it break on the first error.
Option loc is set to true, which means that the nodes have a line and column-based location info.
This way, when an error occurs, the linenumber and position on that line are properties of the returned object.
Range is an option to supply the nodes with an index-based location range. It is set to true.
The raw option is set to true. This means that literals have extra property which stores the verbatim source
There is an extra array created in the returning object that contains all found tokens.
This is set by the option ``tokens'', and is set to true.
Comment is not set, so default to false.
This an extra array containing all line and block comments.

Although there are a lot of metrics calculated, the use of them are beyond the
use in our project.
The metrics of two fully different programs can be the same.
This because that metric can be a mathematical calculation of lines and statements,
thus have no meaning of the content of the code.


\section{Syntax}

Checking the syntax is done by calling the Esprima 
parser\footnote{\url{http://esprima.org/}}. This is a light and fast parser, 
which returns an abstract syntax tree (AST) if there is no error, otherwise it returns
the error. The configuration of the parser can be set within the check.
The AST that is produced is returned to the GUI, thus giving the \gls{student} a
graphical representation of the code he submitted.
For a detailed summary of the options, see the Maintainability section.
In this \gls{check} the options are set as follows : tolerant:false, loc: true, range: true, raw: true, tokens: true.
The other options have the default value of false.


\section{Future work}
An extention of the \glspl{check} could be the validation of
every single line of code to confirm that it is valid \gls{js-code}.
Also the AST can be used to do more \glspl{check} and say something about the
equality of the submitted code and the standard solution. Based on the domain 
research article about the Recipe, it could be fruitful to create a \gls{check} 
that is able to determine whether the prescribed recipe for code construction 
has been followed. This might involve an multistage submission approach, where 
each step of the recipe is followed by an examination. Whether this is desirable
might be best answered by the product owners, but the architecture is very well
suited for such kind of interim feedback.

